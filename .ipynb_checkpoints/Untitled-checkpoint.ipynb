{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import pandas as pd\n",
    "# from IPython.display import display, HTML, Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import pyplot\n",
    "# from random import randint\n",
    "import numpy as np\n",
    "# import sys\n",
    "from sklearn import dummy\n",
    "import os\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from statsmodels.stats.contingency_tables import mcnemar\n",
    "# from scipy.stats.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "#%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from superlearner import SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperLearnerClassifier():\n",
    "    \n",
    "    \"\"\"\n",
    "    An ensemble classifier that uses heterogeneous models at the base layer and a aggregation model at the aggregation layer.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    use_stacked_prob : bool, optional (default = False)\n",
    "        Option to use probability estimates rather than classifiacations \n",
    "        for training at the stacked layer.\n",
    "    \n",
    "    stacked_classifier : string or None, optional (default = decision_tree)\n",
    "        Choice of classifier on the stacked dataset Z. Options are: \n",
    "        \"decision_tree\", \"logistic_regression\", \"k_nearest_neighbours\", \n",
    "        \"random_forest\" or \"most_frequent\".\n",
    "        \n",
    "    estimators_to_remove : list, optional (default = [])\n",
    "        Option to remove (in order to specify) one or more of the base\n",
    "        estimators. Choose from: [\"decision_tree\", \"random_forest\", \n",
    "        \"linear_svm\", \"bagging\", \"k_neighbours\", \"logistic_regression\"]\n",
    "        \n",
    "    include_original_input : bool, optional (default = False)\n",
    "        Include original input data, X, at the stacked layer.\n",
    "         \n",
    "         \n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    estimators = A dictionary of the form {\"model_name\": model, ... } \n",
    "        storing all the trained based estimators.\n",
    "        \n",
    "    Z_classifier = sklearn model object of the stacked layer model.\n",
    "        \n",
    "    Z = Pandas DataFrame containing the stacked layer dataset Z (when: use_stacked_prob \n",
    "        = False)\n",
    "        \n",
    "    Z_prob = Pandas DataFrame containing the stacked layer dataset Z (when: \n",
    "        use_stacked_prob = True)\n",
    "    \n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "    ----------\n",
    "    .. [1]  van der Laan, M., Polley, E. & Hubbard, A. (2007). \n",
    "            Super Learner. Statistical Applications in Genetics \n",
    "            and Molecular Biology, 6(1) \n",
    "            doi:10.2202/1544-6115.1309\n",
    "         \n",
    "         \n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> clf = SuperLearnerClassifier()\n",
    "    >>> iris = load_iris()\n",
    "    >>> clf.fit(pd.DataFrame(iris.data), iris.target)\n",
    "    >>> cross_val_score(clf, pd.DataFrame(iris.data), iris.target, cv=10)\n",
    "\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, use_stacked_prob = False, stacked_classifier = \"decision_tree\", estimators_to_remove = [],\n",
    "                include_original_input = False):\n",
    "        \"\"\"Setup a SuperLearner classifier\"\"\"     \n",
    "        self.decision_tree = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=7, min_samples_split=11)\n",
    "        self.random_forest = ensemble.RandomForestClassifier(n_estimators=500, max_features = 4) #change_max_features\n",
    "        self.bagging = ensemble.BaggingClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\"), n_estimators=10)     \n",
    "        self.logistic_model = linear_model.LogisticRegression(multi_class='auto')\n",
    "        self.k_nearest_neighbours = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "        self.linear_svc = svm.SVC(kernel=\"linear\", C=1.0, probability=True)\n",
    "\n",
    "        self.include_original_input = include_original_input\n",
    "        self.use_stacked_prob = use_stacked_prob\n",
    "        \n",
    "        self.estimators = {\"decision_tree\":self.decision_tree, \"random_forest\":self.random_forest,\n",
    "                           \"bagging\":self.bagging, \"logistic_regression\":self.logistic_model,\n",
    "                           \"k_nearest_neighbours\":self.k_nearest_neighbours, \"linear_svc\":self.linear_svc}\n",
    "        \n",
    "        #can use any subset of the availabe estimators\n",
    "        self.estimators = {key: value for key, value in self.estimators.items() if key not in estimators_to_remove}\n",
    "\n",
    "\n",
    "        #stacked layer classifier\n",
    "        if stacked_classifier == \"decision_tree\" or stacked_classifier == None:\n",
    "            self.Z_classifier = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "        elif stacked_classifier == \"logistic_regression\":\n",
    "            self.Z_classifier = linear_model.LogisticRegression()\n",
    "        elif stacked_classifier == \"k_nearest_neighbours\":\n",
    "            self.Z_classifier = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "        elif stacked_classifier == \"random_forest\":\n",
    "            self.Z_classifier = ensemble.RandomForestClassifier(n_estimators= 500)\n",
    "        elif stacked_classifier == \"most_frequent\":\n",
    "            self.Z_classifier = dummy.DummyClassifier(strategy=\"most_frequent\")\n",
    "        else:\n",
    "            raise ValueError('Error: Not known classifier for stacked layer classifier, check spelling')\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Build a SuperLearner classifier from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"    \n",
    "        \n",
    "        results_list = []\n",
    "        results_list_prob = []\n",
    "        \n",
    "        if self.use_stacked_prob == False:  # use classifications at stacked layer\n",
    "            k_fold = KFold(5, shuffle=False, random_state=None)\n",
    "            for k, (train, test) in enumerate(k_fold.split(X, Y)):  # looping through folds\n",
    "                prediction_list = [] \n",
    "                for name, model in self.estimators.items():\n",
    "                    model.fit(X.iloc[train,], Y[train])  # fitting each model to fold training data\n",
    "                    pred = model.predict(X.iloc[test,])  # predicting each model on its folds test data\n",
    "                    prediction_list.append(np.array(pred)) \n",
    "                fold_k = pd.DataFrame(prediction_list) \n",
    "                fold_k = fold_k.T\n",
    "                results_list.append(fold_k) \n",
    "                \n",
    "            self.Z = pd.concat(results_list).reset_index(drop = True)  \n",
    "            self.Z.columns = self.estimators.keys()\n",
    "            \n",
    "            #include original inputs data?\n",
    "            if self.include_original_input:\n",
    "                X.reset_index(drop = True)\n",
    "                self.Z = pd.concat([self.Z, X.reset_index(drop = True)], axis=1, join_axes=[self.Z.index])\n",
    "            \n",
    "            #fit Z_classifier to Z\n",
    "            self.Z_classifier.fit(self.Z, Y)\n",
    "           \n",
    "\n",
    "        elif self.use_stacked_prob:\n",
    "            k_fold = KFold(5, shuffle=False, random_state=None)\n",
    "            for k, (train, test) in enumerate(k_fold.split(X, Y)):  # looping through folds\n",
    "                prediction_df = pd.DataFrame() \n",
    "                for name, model in self.estimators.items(): \n",
    "                    model.fit(X.iloc[train,], Y[train])  # fitting each model to fold training data\n",
    "                    pred_prob = model.predict_proba(X.iloc[test,])  # predicting each model on its folds test data\n",
    "                    pred_prob = pd.DataFrame(pred_prob) \n",
    "                    prediction_df = pd.concat([prediction_df, pred_prob], axis=1) \n",
    " \n",
    "                fold_k_prob = prediction_df \n",
    "                results_list_prob.append(fold_k_prob) \n",
    "\n",
    "            self.Z_prob = pd.concat(results_list_prob).reset_index(drop = True)  \n",
    "            \n",
    "            #include original inputs data?\n",
    "            if self.include_original_input:\n",
    "                X.reset_index(drop = True)\n",
    "                self.Z_prob = pd.concat([self.Z_prob, X.reset_index(drop = True)], axis=1, join_axes=[self.Z_prob.index])\n",
    "            \n",
    "            #fit decision tree to Z\n",
    "            self.Z_classifier.fit(self.Z_prob, Y)\n",
    "\n",
    "        #Now retrain all estimators using full dataset\n",
    "        self.estimators = {key: model.fit(X,Y) for key, model in self.estimators.items()}\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, ].\n",
    "            The predicted class labels of the input samples. \n",
    "        \"\"\"\n",
    "        \n",
    "        results_list = []\n",
    "        \n",
    "        if self.use_stacked_prob == False:\n",
    "            for name, model in self.estimators.items():\n",
    "                results_list.append(np.array(model.predict(X)))\n",
    "\n",
    "            stacked_layer = pd.DataFrame(results_list).T\n",
    "            #check if predicting on input values too\n",
    "            if self.include_original_input == False:\n",
    "                pass\n",
    "            else:\n",
    "                X.reset_index(drop = True)\n",
    "                stacked_layer = pd.concat([stacked_layer, X.reset_index(drop = True)], axis=1, join_axes=[stacked_layer.index])\n",
    "\n",
    "        elif self.use_stacked_prob == True:\n",
    "            for name, model in self.estimators.items():\n",
    "                results_list.append(pd.DataFrame(model.predict_proba(X)))\n",
    "        \n",
    "            stacked_layer = pd.concat(results_list, axis=1)\n",
    "            #check if predicting on input values too\n",
    "            if self.include_original_input == False:\n",
    "                pass\n",
    "            else:\n",
    "                X.reset_index(drop = True)\n",
    "                stacked_layer = pd.concat([stacked_layer, X.reset_index(drop = True)], axis=1, join_axes=[stacked_layer.index])\n",
    "        \n",
    "        return self.Z_classifier.predict(stacked_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933333333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model is working on iris dataset\n",
    "clf = SuperLearnerClassifier(use_stacked_prob=False)\n",
    "iris = load_iris()\n",
    "clf.fit(pd.DataFrame(iris.data), iris.target)\n",
    "accuracy_score(clf.predict(pd.DataFrame(iris.data)), iris.target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "super_learner_env",
   "language": "python",
   "name": "super_learner_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
